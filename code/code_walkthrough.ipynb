{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54740e5f",
   "metadata": {},
   "source": [
    "# 1. One time Data Loading -  scripts/data_processing.py\n",
    "\n",
    "    - We are loading Users and Business Data as registered entities and saving in parquet format in S3.\n",
    "    - Other streaming datasets we are loading some part of it, so the system will have base data\n",
    "    - Review (any business place), Checkin and Tips data will be processed as stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943be12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from sampling import *\n",
    "from sentiment import *\n",
    "\n",
    "global sample\n",
    "\n",
    "\n",
    "baseInputPath = baseInputPath\n",
    "\n",
    "\n",
    "def process_user_data(spark, sample):\n",
    "    \"\"\"\n",
    "    This function processes user data in PySpark performs transformations,\n",
    "    writes to Parquet, and finally returns the user DataFrame.\n",
    "    It uses caching by checking if the parquet file exists.\n",
    "\n",
    "    if sampling is defined, it will join the sampled users with the user data, to speed up the process.\n",
    "\n",
    "    It will read all the users data registered in the yelp platform and do the required transformations like\n",
    "        - converting the yelping_since column to timestamp\n",
    "        - making the elite column as array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        userDf = spark.read.parquet(f\"{sample_output_path(sample)}/user\") \n",
    "    except Exception as e:\n",
    "\n",
    "        sampled_users, is_sampled = get_sampled_users_data(spark, sample)\n",
    "        userDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json')\n",
    "        if is_sampled:\n",
    "            userDf = userDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "        userDf = userDf \\\n",
    "            .drop(\"friends\") \\\n",
    "            .withColumn(\"elite\", split(col(\"elite\"), \", \")) \\\n",
    "            .withColumn(\"yelping_since\", col(\"yelping_since\").cast(\"timestamp\"))\n",
    "\n",
    "        userDf.repartition(1).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/user\")\n",
    "        userDf = spark.read.parquet(f\"{sample_output_path(sample)}/user\")\n",
    "        print(f\"sample users ares = {userDf.count()}\")\n",
    "    return userDf\n",
    "\n",
    "\n",
    "def process_business_data(spark, sample):\n",
    "    \"\"\"\n",
    "    This function processes user data in PySpark performs transformations,\n",
    "    writes to Parquet, and finally returns the business DataFrame.\n",
    "    It uses caching by checking if the parquet file exists.\n",
    "\n",
    "    it will do the following transformations:\n",
    "        - convert the categories column to array (this will help us to do the aggregation on the categories)\n",
    "        - convert the hours column to map\n",
    "        - convert the attributes column to map\n",
    "        - etc.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        businessDf = spark.read.parquet(f\"{sample_output_path(sample)}/business\")\n",
    "    except Exception as e:\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"attributes\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"hours\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"is_open\", LongType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"review_count\", LongType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "        sampled_business, is_sampled = get_sampled_business_data(spark, sample)\n",
    "        businessDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_business.json', schema)\n",
    "        if is_sampled:\n",
    "            businessDf = businessDf.join(sampled_business, on = [\"business_id\"])\n",
    "\n",
    "        businessDf = businessDf \\\n",
    "            .withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "        businessDf.repartition(2).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/business\")\n",
    "        businessDf = spark.read.parquet(f\"{sample_output_path(sample)}/business\")\n",
    "        print(f\"sample business ares = {businessDf.count()}\")\n",
    "\n",
    "    return businessDf\n",
    "\n",
    "\n",
    "def process_friends_data(spark, sample):\n",
    "    \"\"\"\n",
    "    This function processes friends data in Spark, attempting to read a parquet file,\n",
    "    and in case of an exception, retrieves sampled user data, joins it if sampled, selects\n",
    "    relevant columns, prints the schema, writes the data to parquet, reads it again, and finally\n",
    "    returns the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        friendsDf = spark.read.parquet(f\"{sample_output_path(sample)}/friends\")\n",
    "    except Exception as e:\n",
    "\n",
    "        sampled_users, is_sampled = get_sampled_users_data(spark, sample)\n",
    "        friendsDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_user.json')\n",
    "        if is_sampled:\n",
    "            friendsDf = friendsDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "        friendsDf = friendsDf.select(\"user_id\", split(col(\"friends\"), \", \").alias(\"friends\"))\n",
    "\n",
    "        friendsDf.printSchema()\n",
    "        friendsDf.repartition(2).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/friends\")\n",
    "        friendsDf = spark.read.parquet(f\"{sample_output_path(sample)}/friends\")\n",
    "        print(\"sample friends ares = \", friendsDf.count())\n",
    "    return friendsDf\n",
    "\n",
    "\n",
    "# def process_checkin_data(spark, sample):\n",
    "#     \"\"\"\n",
    "#     these are one time historical data load, so system will have some initial data, and later it\n",
    "#     will come as stream.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         checkinDf = spark.read.parquet(f\"{sample_output_path(sample)}/checkin\")\n",
    "#     except Exception as e:\n",
    "\n",
    "#         sampled_business, is_sampled = get_sampled_business_data(spark, sample)\n",
    "#         checkinDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_checkin.json')\n",
    "#         if is_sampled:\n",
    "#             checkinDf = checkinDf.join(sampled_business, on = [\"business_id\"])\n",
    "\n",
    "#         checkinDf = checkinDf \\\n",
    "#             .withColumn(\"date\", expr(\"transform(split(date, ', '), d -> to_timestamp(d))\").cast(ArrayType(TimestampType())))\n",
    "\n",
    "#         checkinDf.printSchema()\n",
    "\n",
    "#         checkinDf.repartition(1).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/checkin\")\n",
    "#         checkinDf = spark.read.parquet(f\"{sample_output_path(sample)}/checkin\")\n",
    "#         print(\"sample checkin ares = \", checkinDf.count())\n",
    "#     return checkinDf\n",
    "\n",
    "\n",
    "# def process_tip_data(spark, sample):\n",
    "#     \"\"\"\n",
    "#     these are one time historical data load, so system will have some initial data, and later it\n",
    "#     will come as stream.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         tipDf = spark.read.parquet(f\"{sample_output_path(sample)}/tip\")\n",
    "#     except Exception as e:\n",
    "\n",
    "#         sampled_users, is_sampled = get_sampled_users_data(spark, sample)\n",
    "#         tipDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_tip.json')\n",
    "#         if is_sampled:\n",
    "#             tipDf = tipDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "#         tipDf = tipDf.withColumn(\"date\", col(\"date\").cast(\"timestamp\"))\n",
    "\n",
    "#         tipDf.repartition(1).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/tip\")\n",
    "#         tipDf = spark.read.parquet(f\"{sample_output_path(sample)}/tip\")\n",
    "#         print(\"sample tip ares = \", tipDf.count())\n",
    "#     return tipDf\n",
    "\n",
    "\n",
    "# def process_review_data(spark, sample):\n",
    "#     \"\"\"\n",
    "#     these are one time historical data load, so system will have some initial data, and later it\n",
    "#     will come as stream.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         reviewDf = spark.read.parquet(f\"{sample_output_path(sample)}/review\")\n",
    "#     except Exception as e:\n",
    "#         sampled_users, is_sampled = get_sampled_users_data(spark, sample)\n",
    "#         reviewDf = spark.read.json(f'{baseInputPath}/yelp_academic_dataset_review.json')\n",
    "#         if is_sampled:\n",
    "#             reviewDf = reviewDf.join(sampled_users, on = [\"user_id\"])\n",
    "\n",
    "#         reviewDf = reviewDf \\\n",
    "#             .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "#             .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "#             .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "#         reviewDf.printSchema()\n",
    "#         reviewDf.repartition(4).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/review\")\n",
    "#         reviewDf = spark.read.parquet(f\"{sample_output_path(sample)}/review\")\n",
    "#         print(\"sample review ares = \", reviewDf.count())\n",
    "#     return reviewDf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: data_processing.py <baseInputPath> <baseOutputPath> <sample>\")\n",
    "        exit(-1)\n",
    "\n",
    "    baseInputPath = sys.argv[1]\n",
    "    baseOutputPath = sys.argv[2]\n",
    "    sample = float(sys.argv[3])\n",
    "\n",
    "    sparkSession = init_spark()\n",
    "    user_df = process_user_data(sparkSession, sample)\n",
    "    business_df = process_business_data(sparkSession, sample)\n",
    "    friends_df = process_friends_data(sparkSession, sample)\n",
    "    checkin_df = process_checkin_data(sparkSession, sample)\n",
    "    tip_df = process_tip_data(sparkSession, sample)\n",
    "    review_df = process_review_data(sparkSession, sample)\n",
    "\n",
    "    sparkSession.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded91ea5",
   "metadata": {},
   "source": [
    "# 2. Sentiment analysis and word tokenization - scripts/sentiment.py\n",
    "    \n",
    "    - This is a class which have all the natural language processing functions.\n",
    "    - This class is used to perform sentiment analysis and tokenize the words.\n",
    "    - It uses NLTK python library for that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    This method will return the sentiment of the text.\n",
    "    It will use the nltk vader sentiment analyzer.\n",
    "    it will return the sentiment as positive, negative or neutral.\n",
    "    \"\"\"\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def tokenize_and_get_top_words(text, sample_size=0.0001):\n",
    "    \"\"\"\n",
    "    This method will tokenize the text and will return the top 10 words.\n",
    "    it will also remove the stop words to capture the most important words.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    top_words = [word  for word, k in freq_dist.most_common(10)]\n",
    "    return top_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c31a5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Streaming Data Ingestion - scripts/data_ingestion.py\n",
    "\n",
    "    - this module will handle the incremental data loading from Kafka brokers.\n",
    "    - the process will consume review, tips, checkin data from kafka, as the users and business will be    registered but these 3 data sets are dynamic.\n",
    "    - We will read data from kafka, enrich the batch and then write it 'append' mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc1553",
   "metadata": {},
   "source": [
    "    -  This is the main method, which will be called when the program will be executed.\n",
    "    - it will read the arguments from the command line.\n",
    "    - it will also initialize the spark session.\n",
    "    - it will also call the consumer class to read the data from the kafka topic.\n",
    "    - it will call the methods to read the data from the kafka topic.\n",
    "    - it will also call the methods to write the data to the parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e149b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "\n",
    "from commons import *\n",
    "from sampling import *\n",
    "from sentiment import *\n",
    "\n",
    "global sample\n",
    "\n",
    "class Consumer:\n",
    "    \"\"\"\n",
    "    - This class will consume the data from kafka and write to the parquet files.\n",
    "    - While writing to the parquet files, it will also perform some transformations.\n",
    "    - It will also perform sampling if the sample is specified.\n",
    "    - it will also perform sentiment analysis and tokenize the words to\n",
    "    - capture the most frequent words and sentiment of the review.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, server, output_path):\n",
    "        self.server = server\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def read_from_topic(self, spark, topic):\n",
    "        \"\"\"\n",
    "        Reading Data from the kafka topic.\n",
    "        \"\"\"\n",
    "        print(f\"reading data from the topic = \", topic, \"server = \", self.server)\n",
    "        df = (\n",
    "            spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", self.server)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"subscribe\", topic)\n",
    "            .load()\n",
    "        )\n",
    "       \n",
    "    \n",
    "    print(f\"is spark is reading the streams from kafka = {df.isStreaming}\")\n",
    "        df.printSchema()\n",
    "        return df.withColumn(\"json_string\", col(\"value\").cast(StringType()))\n",
    "\n",
    "    \n",
    "    \n",
    "    def write_stream(self, df_result, topic_name):\n",
    "        \"\"\"\n",
    "        Write stream data to the parquet files.\n",
    "        it writes data as append mode, to avoid whole data rewrite.\n",
    "        we are using trigger to write data every 10 seconds.\n",
    "        \"\"\"\n",
    "        writer = df_result \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"parquet\") \\\n",
    "            .option(\"path\", f\"{self.output_path}/{topic_name}/data\") \\\n",
    "            .option(\"checkpointLocation\", f\"{self.output_path}/{topic_name}/checkpoint\") \\\n",
    "            .trigger(processingTime=\"10 seconds\") \\\n",
    "            .start()\n",
    "        writer.awaitTermination()\n",
    "\n",
    "        \n",
    "        \n",
    "    def read_checkins(self, spark):\n",
    "        \"\"\"\n",
    "        This method will read the checkins data from the kafka topic.\n",
    "        it also performs the transformation on the data, by converting the json string to the dataframe.\n",
    "        \"\"\"\n",
    "        topicName = \"checkins\"\n",
    "        stream_df = self.read_from_topic(spark, topicName)\n",
    "        schema = StructType([\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "        ])\n",
    "        df_result = stream_df.select(from_json(col(\"json_string\"), schema).alias(\"data\"))\n",
    "        self.write_stream(df_result, topicName)\n",
    "\n",
    "    def read_tips(self, spark):\n",
    "        \"\"\"\n",
    "        This method will read the tips data from the kafka topic.\n",
    "        it also performs the transformation on the data, by converting the json string to the dataframe.\n",
    "        \"\"\"\n",
    "        topicName = \"tips\"\n",
    "        stream_df = self.read_from_topic(spark, topicName)\n",
    "        schema = StructType([\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"compliment_count\", LongType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "        ])\n",
    "        df_result = stream_df.select(from_json(col(\"json_string\"), schema).alias(\"data\"))\n",
    "        self.write_stream(df_result, topicName)\n",
    "\n",
    "    def process_review_data_df(self, review_df, x):\n",
    "        \"\"\"\n",
    "        This method will process the review data batch.\n",
    "        each batch will be processed and written to the parquet files.\n",
    "        a batch is the new data that is read from the kafka topic for the first time.\n",
    "        it will do following things:\n",
    "\n",
    "            - perform sentiment analysis on the review text.\n",
    "            - tokenize the words and get the most frequent words.\n",
    "            - write the data to the parquet files.\n",
    "\n",
    "        As this is a generic method, it will be called for each batch, and more and more functionality can be added to it.\n",
    "        \"\"\"\n",
    "        sampled_users, is_sampled = get_sampled_users_data(spark, sample)\n",
    "        if is_sampled:\n",
    "            print(\"got sampled users ... processing that.\")\n",
    "            sampled_users.printSchema()\n",
    "            review_df.printSchema()\n",
    "            review_df = review_df.join(sampled_users, on=[\"user_id\"])\n",
    "\n",
    "        review_df = review_df \\\n",
    "            .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "            .withColumn(\"sentiment\",  get_sentiment(col(\"text\"))) \\\n",
    "            .withColumn(\"frequent_words\", tokenize_and_get_top_words(col(\"text\")))\n",
    "\n",
    "        review_df.printSchema()\n",
    "        review_df.repartition(1).write.mode(\"append\").parquet(f\"{sample_output_path(sample)}/review\")\n",
    "        print(\"sample review ares = \", review_df.count())\n",
    "        return review_df\n",
    "\n",
    "    def read_reviews(self, spark):\n",
    "        \"\"\"\n",
    "        This method will read the review data from the kafka topic.\n",
    "        it also performs the transformation on the data, by converting the json string to the dataframe.\n",
    "        it will use the function process_review_data_df to process each batch.\n",
    "        \"\"\"\n",
    "        topicName = \"reviews\"\n",
    "        schema = StructType([\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"cool\", LongType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"funny\", LongType(), True),\n",
    "            StructField(\"review_id\", StringType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"useful\", LongType(), True),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "        ])\n",
    "        stream_df = self.read_from_topic(spark, topicName)\n",
    "        df_result = stream_df.select(from_json(col(\"json_string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "        writer = df_result.writeStream.outputMode(\"append\").foreachBatch(self.process_review_data_df).start()\n",
    "        writer.awaitTermination(30)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if len(sys.argv) >= 4:\n",
    "        server = sys.argv[1]\n",
    "        topic = sys.argv[2]\n",
    "        output_path = sys.argv[3]\n",
    "        sample = float(sys.argv[4])\n",
    "        spark = init_spark()\n",
    "        consumer = Consumer(server, output_path)\n",
    "        consumer.read_reviews(spark)\n",
    "        consumer.read_tips(spark)\n",
    "        consumer.read_checkins(spark)\n",
    "        spark.stop()\n",
    "    else:\n",
    "        print(\"Invalid number of arguments. Please pass the server and topic name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07612970",
   "metadata": {},
   "source": [
    "# 4. Business Related Attributes - scripts/attributes/business.py\n",
    "\n",
    "    - We are extracting following attributes from business :\n",
    "    \n",
    "        - Different Business categories and how frequently a user will  visit them.\n",
    "        - User preferences for different categories.\n",
    "        - It captures frequency as well as strars.\n",
    "        \n",
    "        - Businesses local will help to identify the users location.\n",
    "        - It will also help to get popular travel destinations for a users, which can be used for travel related segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f41f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import collect_list, avg\n",
    "from pyspark.sql.functions import explode, create_map\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.types import IntegerType, MapType, FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), IntegerType()))\n",
    "def merge_maps_array(map_array):\n",
    "    \"\"\"\n",
    "    Its an UDF to merge the maps in the array.\n",
    "    It will help tp bring a generalize format of the data and will make then earlier to process.\n",
    "\n",
    "    Input data will be\n",
    "        [{\"Active Life\": 1, \"Arts & Entertainment\": 1,\n",
    "        \"Automotive\": 1, \"Beauty & Spas\": 1, \"Education\": 1}]\n",
    "    Output data will be\n",
    "        {\"Active Life\": 1, \"Arts & Entertainment\": 1,\n",
    "        \"Automotive\": 1, \"Beauty & Spas\": 1, \"Education\": 1}\n",
    "\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for m in map_array:\n",
    "        for k, v in m.items():\n",
    "            result[k] = result.get(k, 0) + v\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_customer_category_counts(review_df, business_df):\n",
    "    \"\"\"\n",
    "     - This method will get the customer category counts.\n",
    "     - customer category count will be the count of the categories that the customer has visited.\n",
    "        its important to capture the categories that the customer has visited,\n",
    "        as it will help us to understand the customer's interest in the business.\n",
    "     - number of categories will be the size of the map.\n",
    "\n",
    "    \"\"\"\n",
    "    df = review_df.select(\"user_id\", \"business_id\") \\\n",
    "        .join(business_df.select(\"business_id\", \"categories\"), on=[\"business_id\"]) \\\n",
    "        .select(\"user_id\", explode(\"categories\").alias(\"category\")) \\\n",
    "        .groupBy(\"user_id\", \"category\").count() \\\n",
    "        .withColumn(\"category_map\", create_map(col(\"category\"), col(\"count\"))) \\\n",
    "        .groupBy(\"user_id\").agg(collect_list(col(\"category_map\")).alias(\"category_map\")) \\\n",
    "        .withColumn(\"category_map\", merge_maps_array(col(\"category_map\"))) \\\n",
    "        .select(\"user_id\", \"category_map\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), FloatType()))\n",
    "def merge_maps_array_float(map_array):\n",
    "    \"\"\"\n",
    "    Its an UDF to merge the maps in the array.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for m in map_array:\n",
    "        for k, v in m.items():\n",
    "            result[k] = result.get(k, 0) + v\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_customer_category_avg_rating(review_df, business_df):\n",
    "    \"\"\"\n",
    "    - This method will get the customer category avg rating.\n",
    "    - customer category avg rating will be the avg of the ratings that the customer has given to the business.\n",
    "    - number of categories will be the size of the map.\n",
    "    \"\"\"\n",
    "    df = review_df.select(\"user_id\", \"business_id\", \"stars\") \\\n",
    "        .filter(col(\"stars\").isNotNull()) \\\n",
    "        .join(business_df.select(\"business_id\", \"categories\"), on=[\"business_id\"]) \\\n",
    "        .select(\"user_id\", \"stars\", explode(\"categories\").alias(\"category\")) \\\n",
    "        .groupBy(\"user_id\", \"category\").agg(avg(col(\"stars\")).alias(\"avg_stars\")) \\\n",
    "        .withColumn(\"category_map\", create_map(col(\"category\"), col(\"avg_stars\"))) \\\n",
    "        .groupBy(\"user_id\").agg(collect_list(col(\"category_map\")).alias(\"category_map\")) \\\n",
    "        .withColumn(\"category_avg_stars\", merge_maps_array_float(col(\"category_map\"))) \\\n",
    "        .select(\"user_id\", \"category_avg_stars\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def home_city(items):\n",
    "    \"\"\"\n",
    "    This method will get the home city of the customer.\n",
    "    It uses the location frequency where customer uses services and pick the most frequent city.\n",
    "    \"\"\"\n",
    "    if items is None:\n",
    "        return None\n",
    "    return max(items, key=lambda x: items.count(x))\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), IntegerType()))\n",
    "def traveling_city(items, home):\n",
    "    \"\"\"\n",
    "    This method will get the traveling map.\n",
    "    Traveling map will be the map of the cities that the customer has visited.\n",
    "    \"\"\"\n",
    "    if items is None:\n",
    "        return None\n",
    "    travel_map = {}\n",
    "    for i in items:\n",
    "        if i == home:\n",
    "            continue\n",
    "        travel_map[i] = travel_map.get(i, 0) + 1\n",
    "    return travel_map\n",
    "\n",
    "\n",
    "def get_customer_area(review_df, business_df):\n",
    "    \"\"\"\n",
    "    this method will get the customer geographical area data with latitude and longitude.\n",
    "    it also captures the home city and the other traveling cities and how frequently user visits them\n",
    "    \"\"\"\n",
    "    df = review_df.select(\"user_id\", \"business_id\") \\\n",
    "        .join(business_df.select(\"business_id\", \"city\"), on=[\"business_id\"]) \\\n",
    "        .select(\"user_id\", \"city\") \\\n",
    "        .groupBy(\"user_id\").agg(collect_list(col(\"city\")).alias(\"city_freq\")) \\\n",
    "        .withColumn(\"city\", home_city(col(\"city_freq\"))) \\\n",
    "        .withColumn(\"travel_map\", traveling_city(col(\"city_freq\"), col(\"city\"))) \\\n",
    "        .select(\"user_id\", \"city\", \"travel_map\") \\\n",
    "        .join(business_df.select(\"city\", \"latitude\", \"longitude\").dropDuplicates([\"city\"]), on=[\"city\"]) \\\n",
    "        .select(\"user_id\", \"city\", \"travel_map\", \"latitude\", \"longitude\") \\\n",
    "        .withColumnRenamed(\"city\", \"user_city\") \\\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fe4f8",
   "metadata": {},
   "source": [
    "# 5. Review Related Attributes - scripts/attributes/review.py\n",
    "\n",
    "    - Here we can capture, sentiment of a review, and how frequently user gives them.\n",
    "    - We can also find which key-word user uses most to get specific information like: even user dont like place he can say fries are great here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe26566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import explode, create_map\n",
    "from pyspark.sql.types import IntegerType, MapType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), IntegerType()))\n",
    "def merge_maps_array(map_array):\n",
    "    \"\"\"\n",
    "    Its an UDF to merge the maps in the array.\n",
    "    It will help tp bring a generalize format of the data and will make then earlier to process.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for m in map_array:\n",
    "        for k, v in m.items():\n",
    "            result[k] = result.get(k, 0) + v\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_sentiments_count(review_df):\n",
    "    \"\"\"\n",
    "    - This method will get the sentiment count of the user.\n",
    "    - sentiment count will be the count of the sentiment that the customer has given.\n",
    "    - its important to capture the sentiment that the customer has given, and what the ratio of the sentiment is.\n",
    "    \"\"\"\n",
    "    df = review_df.select(\"user_id\", \"sentiment\").groupBy(\"user_id\", \"sentiment\").count() \\\n",
    "        .withColumnRenamed(\"count\", \"sentiment_count\") \\\n",
    "        .withColumn(\"sentiment_map\", create_map(col(\"sentiment\"), col(\"sentiment_count\"))) \\\n",
    "        .groupBy(\"user_id\").agg(collect_list(col(\"sentiment_map\")).alias(\"sentiment_map\")) \\\n",
    "        .withColumn(\"sentiment_map\", merge_maps_array(col(\"sentiment_map\")))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def most_frequent_words(review_df):\n",
    "    \"\"\"\n",
    "    - This method will get the most frequent words used by the user.\n",
    "    - most frequent words will be the words that the customer has used the most.\n",
    "    - its important to capture the most frequent words used by the customer\n",
    "    \"\"\"\n",
    "    return review_df.select(\"user_id\", explode(\"frequent_words\").alias(\"frequent_words\")) \\\n",
    "        .groupBy(\"user_id\", \"frequent_words\").count() \\\n",
    "        .withColumn(\"frequent_words_map\", create_map(col(\"frequent_words\"), col(\"count\"))) \\\n",
    "        .groupBy(\"user_id\").agg(collect_list(col(\"frequent_words_map\")).alias(\"frequent_words_map\")) \\\n",
    "        .withColumn(\"frequent_words_map\", merge_maps_array(col(\"frequent_words_map\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7744b9",
   "metadata": {},
   "source": [
    "# 6. User Related Attributes  - scripts/attributes/users_agg.py\n",
    "\n",
    "    - Compute differnt user level attributes.\n",
    "        - avg_rating : average rating given by the customer\n",
    "        - min_stars : minimum rating given by the customer\n",
    "        - max_stars : maximum rating given by the customer\n",
    "        \n",
    "        - first_seen :  when the customer put their first review\n",
    "        - last_seen : when the customer put their last review\n",
    "        \n",
    "        - date_diff : how many days the customer has been inactive. (last_seen - first_seen)\n",
    "                      this can be used for customer retention.\n",
    "        \n",
    "        - different_business_count : number of different business the customer has visited\n",
    "                                     it captures how customers is using platform.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "\n",
    "def get_friends_count(friends_df):\n",
    "    \"\"\"\n",
    "    This method will get the friends count of the user.\n",
    "    friends count will be the count of the friends that the customer has.\n",
    "    it captures how many friends the customer has and it's important to know if the customer is social or not\n",
    "    and how big their social circle is.\n",
    "    \"\"\"\n",
    "    df = friends_df.select(\"user_id\", size(col(\"friends\")).alias(\"friends_count\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_customer_agg_value(spark, review_df):\n",
    "    \"\"\"\n",
    "    This method will get the customer aggregation value.\n",
    "    customer aggregation value will be the aggregation of the customers different attributes.\n",
    "    it will compute attributes like:\n",
    "        - first_seen :  when the customer put their first review\n",
    "        - last_seen : when the customer put their last review\n",
    "        - date_diff : how many days the customer has been inactive. (last_seen - first_seen)\n",
    "                      this can be used for customer retention.\n",
    "        - different_business_count : number of different business the customer has visited\n",
    "                                     it captures how customers is using platform.\n",
    "        - avg_rating : average rating given by the customer\n",
    "        - min_stars : minimum rating given by the customer\n",
    "        - max_stars : maximum rating given by the customer\n",
    "\n",
    "    \"\"\"\n",
    "    review_df.createOrReplaceTempView(\"review\")\n",
    "    return spark.sql(\"\"\"\n",
    "                        select \n",
    "                            user_id, \n",
    "                            min(date) as first_seen, \n",
    "                            max(date) as last_seen, \n",
    "                            DATEDIFF(max(date), min(date)) as date_diff,\n",
    "                            count(distinct business_id) as different_business_count,\n",
    "                            avg(stars) as avg_rating,\n",
    "                            min(stars) as min_stars,\n",
    "                            max(stars) as max_stars\n",
    "                        from review\n",
    "                        group by user_id \n",
    "                    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45812a8",
   "metadata": {},
   "source": [
    "# 7. Feature Aggregation - scripts/feature_aggregator.py \n",
    "\n",
    "    - It merges all the attributes in a single table which will have so many dimensions.\n",
    "    - Each dimension will be independent of each other.\n",
    "    \n",
    "    - Complex data types like Map will captures multiple dimensions for users. Example\n",
    "        A user can on average give 4.3 stars for his yelp review but \n",
    "        category_avg_stars = {category -> rating}, will capture that at more granular level and category can be treated as other dimension.\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b95b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from data_processing import *\n",
    "\n",
    "from attributes.users_agg import *\n",
    "from attributes.business import *\n",
    "from attributes.reviews import *\n",
    "\n",
    "from storage import *\n",
    "\n",
    "\n",
    "def merge_attributes(spark, sample):\n",
    "    \"\"\"\n",
    "    - This method will merge all the attributes of the user.\n",
    "    - it will merge all the attributes of the user and will create a single dataframe.\n",
    "    - it will be used to create the customer profile and for our use case customer is the key\n",
    "    \"\"\"\n",
    "    user_df = process_user_data(spark, sample)\n",
    "    business_df = process_business_data(spark, sample)\n",
    "    friends_df = process_friends_data(spark, sample)\n",
    "    checkin_df = process_checkin_data(spark, sample)\n",
    "    tip_df = process_tip_data(spark, sample)\n",
    "    review_df = process_review_data(spark, sample)\n",
    "\n",
    "    avg_catg_start_df = get_customer_category_avg_rating(review_df, business_df)\n",
    "    customer_area_df = get_customer_area(review_df, business_df)\n",
    "    user_agg_df = get_customer_agg_value(spark, review_df)\n",
    "    user_category_df = get_customer_category_counts(review_df, business_df)\n",
    "    friends_count_df = get_friends_count(friends_df)\n",
    "    sentiment_count_df = get_sentiments_count(review_df)\n",
    "    frequent_words_df = most_frequent_words(review_df)\n",
    "\n",
    "    complete_user_df = user_df \\\n",
    "        .join(user_agg_df, on=[\"user_id\"]) \\\n",
    "        .join(user_category_df, on=[\"user_id\"]) \\\n",
    "        .join(friends_count_df, on=[\"user_id\"]) \\\n",
    "        .join(sentiment_count_df, on=[\"user_id\"]) \\\n",
    "        .join(frequent_words_df, on=[\"user_id\"]) \\\n",
    "        .join(customer_area_df, on=[\"user_id\"]) \\\n",
    "        .join(avg_catg_start_df, on=[\"user_id\"])\n",
    "\n",
    "    complete_user_df.printSchema()\n",
    "    # complete_user_df.show()\n",
    "    print(\"total counts are = \", complete_user_df.count(), \"  user counts = \", user_df.count())\n",
    "    \n",
    "    #datacube\n",
    "    complete_user_df.repartition(4).write.mode(\"overwrite\").parquet(f\"{sample_output_path(sample)}/combined\")\n",
    "    return spark.read.parquet(f\"{sample_output_path(sample)}/combined\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: user_attributes.py <sample>\")\n",
    "        exit(-1)\n",
    "\n",
    "    sample = float(sys.argv[1])\n",
    "    sparkSession = init_spark()\n",
    "    merged_df = merge_attributes(sparkSession, sample)\n",
    "    save_spark_df_to_db(merged_df, \"users\")\n",
    "    sparkSession.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7284b",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a358bca",
   "metadata": {},
   "source": [
    "### Streamlit App Link : https://yelp-customer-segmentation.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dbcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b9229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa102d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925e950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece771f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35830c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869317bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff28d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb4b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
